{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9e9cc7",
   "metadata": {},
   "source": [
    "1) Load (typed) + quick inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90e1624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py7zr\n",
    "from pathlib import Path\n",
    "\n",
    "raw_dir = Path(\"data/raw\")\n",
    "\n",
    "for file in raw_dir.glob(\"*.7z\"):\n",
    "    with py7zr.SevenZipFile(file, mode='r') as archive:\n",
    "        archive.extractall(path=raw_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566e03b",
   "metadata": {},
   "source": [
    "### Load and inspect shapes/types and missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be2f803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (125497040, 5)\n",
      "        date  store_nbr  item_nbr  unit_sales onpromotion\n",
      "0 2013-01-01         25    103665         7.0         NaN\n",
      "1 2013-01-01         25    105574         1.0         NaN\n",
      "2 2013-01-01         25    105575         2.0         NaN\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125497040 entries, 0 to 125497039\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Dtype         \n",
      "---  ------       -----         \n",
      " 0   date         datetime64[ns]\n",
      " 1   store_nbr    int16         \n",
      " 2   item_nbr     int32         \n",
      " 3   unit_sales   float32       \n",
      " 4   onpromotion  object        \n",
      "dtypes: datetime64[ns](1), float32(1), int16(1), int32(1), object(1)\n",
      "memory usage: 3.0+ GB\n",
      "None\n",
      "Date range: 2013-01-01 00:00:00 → 2017-08-15 00:00:00\n",
      "Missing values (top):\n",
      " onpromotion    21657651\n",
      "date                  0\n",
      "store_nbr             0\n",
      "item_nbr              0\n",
      "unit_sales            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "PATH = r\"C:\\Users\\Yizi\\New folder\\Payday-surge-favorita\\data\\raw\\train.csv\"\n",
    "\n",
    "USECOLS = [\"date\",\"store_nbr\",\"item_nbr\",\"unit_sales\",\"onpromotion\"]\n",
    "DTYPES  = {\"store_nbr\":\"int16\",\"item_nbr\":\"int32\",\"unit_sales\":\"float32\"}\n",
    "\n",
    "df = pd.read_csv(PATH, usecols=USECOLS, parse_dates=[\"date\"], dtype=DTYPES, low_memory=False)\n",
    "\n",
    "print(\"Shape:\", df.shape)                 # → rows, cols\n",
    "print(df.head(3))                         # → first rows look sane?\n",
    "print(df.info())                          # → dtypes are as expected\n",
    "print(\"Date range:\", df[\"date\"].min(), \"→\", df[\"date\"].max())\n",
    "\n",
    "na = df.isna().sum().sort_values(ascending=False)\n",
    "print(\"Missing values (top):\\n\", na.head(10))   # → `onpromotion` will have many NaNs (expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f4fab",
   "metadata": {},
   "source": [
    "### 2) Standardize Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a3cf871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onpromotion dtype: bool\n",
      "onpromotion uniques: [False  True]\n",
      "Remaining NA counts:\n",
      " date           0\n",
      "store_nbr      0\n",
      "item_nbr       0\n",
      "unit_sales     0\n",
      "onpromotion    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df[\"onpromotion\"] = (\n",
    "    df[\"onpromotion\"]\n",
    "      .map({True: True, False: False, \"True\": True, \"False\": False, 1: True, 0: False})\n",
    "      .astype(\"boolean\")   # nullable boolean to avoid FutureWarning\n",
    "      .fillna(False)       # NA = not on promotion\n",
    "      .astype(bool)        # final dtype: bool\n",
    ")\n",
    "\n",
    "df = df.dropna(subset=[\"unit_sales\"])  \n",
    "\n",
    "print(\"onpromotion dtype:\", df[\"onpromotion\"].dtype)    # Output: bool\n",
    "print(\"onpromotion uniques:\", df[\"onpromotion\"].unique())  # Output: [True False]\n",
    "print(\"Remaining NA counts:\\n\", df.isna().sum())       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a08f6f",
   "metadata": {},
   "source": [
    "### 3) Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "900df527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate keys before groupby: 0\n",
      "Duplicate keys after groupby: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df[\"was_return\"] = df[\"unit_sales\"] < 0\n",
    "\n",
    "dup_before = df.duplicated(subset=[\"date\",\"store_nbr\",\"item_nbr\"]).sum()\n",
    "print(\"Duplicate keys before groupby:\", dup_before)\n",
    "\n",
    "df = (df.groupby([\"date\",\"store_nbr\",\"item_nbr\"], as_index=False)\n",
    "        .agg(unit_sales=(\"unit_sales\",\"sum\"),\n",
    "             onpromotion=(\"onpromotion\",\"max\"),\n",
    "             was_return=(\"was_return\",\"any\")))\n",
    "\n",
    "dup_after = df.duplicated(subset=[\"date\",\"store_nbr\",\"item_nbr\"]).sum()\n",
    "print(\"Duplicate keys after groupby:\", dup_after)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf0e69f",
   "metadata": {},
   "source": [
    "### 4) Clipping negatives, capped extreme outliers and applied log1p transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c44715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1.254970e+08\n",
      "mean     8.556009e+00\n",
      "std      2.352696e+01\n",
      "min      0.000000e+00\n",
      "50%      4.000000e+00\n",
      "99%      7.100000e+01\n",
      "99.9%    2.127619e+02\n",
      "max      8.944000e+04\n",
      "Name: unit_sales_clipped, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df[\"unit_sales_clipped\"] = df[\"unit_sales\"].clip(lower=0).astype(\"float32\")\n",
    "\n",
    "cap = df[\"unit_sales_clipped\"].quantile(0.999) \n",
    "df[\"unit_sales_capped\"] = np.minimum(df[\"unit_sales_clipped\"], cap).astype(\"float32\")\n",
    "\n",
    "df[\"unit_sales_log1p\"] = np.log1p(df[\"unit_sales_capped\"]).astype(\"float32\")\n",
    "\n",
    "print(df[\"unit_sales_clipped\"].describe(percentiles=[0.99,0.999]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d056d72",
   "metadata": {},
   "source": [
    "### 5) Validation (no null keys, unique keys, clipped ≥ 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a90c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls in keys:\n",
      " date         0\n",
      "store_nbr    0\n",
      "item_nbr     0\n",
      "dtype: int64\n",
      "Duplicate keys: 0\n",
      "Min clipped: 0.0\n",
      "Validation passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Nulls in keys:\\n\", df[[\"date\",\"store_nbr\",\"item_nbr\"]].isna().sum())\n",
    "print(\"Duplicate keys:\", df.duplicated(subset=[\"date\",\"store_nbr\",\"item_nbr\"]).sum())\n",
    "print(\"Min clipped:\", df[\"unit_sales_clipped\"].min())\n",
    "\n",
    "assert df[[\"date\",\"store_nbr\",\"item_nbr\"]].isna().sum().sum() == 0\n",
    "assert df.duplicated(subset=[\"date\",\"store_nbr\",\"item_nbr\"]).sum() == 0\n",
    "assert df[\"unit_sales_clipped\"].min() >= 0.0\n",
    "print(\"Validation passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19d7a9",
   "metadata": {},
   "source": [
    "### 6)  Save to Parquet (partitioned):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c25083de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → data\\processed\\train_clean_parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pyarrow as pa, pyarrow.parquet as pq\n",
    "\n",
    "out_dir = Path(\"data/processed/train_clean_parquet\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df[\"dow\"]   = df[\"date\"].dt.dayofweek.astype(\"int8\")\n",
    "df[\"year\"]  = df[\"date\"].dt.year.astype(\"int16\")\n",
    "df[\"month\"] = df[\"date\"].dt.month.astype(\"int8\")\n",
    "\n",
    "pq.write_to_dataset(\n",
    "    pa.Table.from_pandas(df, preserve_index=False),\n",
    "    root_path=str(out_dir),\n",
    "    partition_cols=[\"year\",\"month\"],\n",
    "    compression=\"zstd\",\n",
    "    use_dictionary=True\n",
    ")\n",
    "print(\"Saved →\", out_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
