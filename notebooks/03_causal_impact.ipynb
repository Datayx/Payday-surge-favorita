{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52cb241c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using processed dir: c:\\Users\\Yizi\\New folder\\Payday-surge-favorita\\data\\processed\n",
      "Rows loaded: 125,497,040 | days after reindex: 1,688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yizi\\New folder\\Payday-surge-favorita\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\Yizi\\New folder\\Payday-surge-favorita\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 59,561\n",
      "Saved figure -> c:\\Users\\Yizi\\New folder\\Payday-surge-favorita\\data\\results\\figures\\sales_forecast.png\n"
     ]
    }
   ],
   "source": [
    "# Notebook 3. Forecasting & Business Application \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# find data/processed \n",
    "def find_processed_dir():\n",
    "    for base in (Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent):\n",
    "        cand = base / \"data\" / \"processed\"\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    raise FileNotFoundError(\"data/processed not found (run from repo root or notebooks/)\")\n",
    "\n",
    "processed = find_processed_dir()\n",
    "print(f\"Using processed dir: {processed}\")\n",
    "\n",
    "# load train data from partitioned folder or single parquet\n",
    "def read_min(path):\n",
    "    try:\n",
    "        return pd.read_parquet(path, columns=[\"date\", \"sales\"])\n",
    "    except Exception:\n",
    "        return pd.read_parquet(path, columns=[\"date\", \"unit_sales\"])\n",
    "\n",
    "def load_train(proc: Path) -> pd.DataFrame:\n",
    "    folder = proc / \"train_clean_parquet\"\n",
    "    if folder.is_dir():\n",
    "        files = sorted(glob(str(folder / \"**\" / \"*.parquet\"), recursive=True))\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No parquet files under {folder}\")\n",
    "        df = pd.concat((read_min(f) for f in files), ignore_index=True)\n",
    "    else:\n",
    "        for name in (\"train_clean.parquet\", \"analysis_ready.parquet\"):\n",
    "            p = proc / name\n",
    "            if p.is_file():\n",
    "                df = read_min(p)\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Missing train parquet (looked for partitioned/, train_clean.parquet, analysis_ready.parquet)\")\n",
    "    if \"sales\" not in df.columns and \"unit_sales\" in df.columns:\n",
    "        df = df.rename(columns={\"unit_sales\": \"sales\"})\n",
    "    return df\n",
    "\n",
    "df = load_train(processed)\n",
    "\n",
    "# Clean and daily aggregate\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"date\"])\n",
    "df[\"sales\"] = pd.to_numeric(df[\"sales\"], errors=\"coerce\").fillna(0.0).astype(\"float64\").clip(lower=0)\n",
    "\n",
    "daily = (df.groupby(\"date\", as_index=False)[\"sales\"].sum().sort_values(\"date\"))\n",
    "\n",
    "# ensure continuous daily index (fill missing days with 0)\n",
    "idx = pd.date_range(daily[\"date\"].min(), daily[\"date\"].max(), freq=\"D\")\n",
    "daily = (daily.set_index(\"date\").reindex(idx, fill_value=0.0).rename_axis(\"date\").reset_index())\n",
    "\n",
    "print(f\"Rows loaded: {len(df):,} | days after reindex: {len(daily):,}\")\n",
    "\n",
    "# Train/test split\n",
    "split_date = pd.Timestamp(\"2017-07-01\")\n",
    "train = daily[daily[\"date\"] < split_date].set_index(\"date\")\n",
    "test  = daily[daily[\"date\"] >= split_date].set_index(\"date\")\n",
    "\n",
    "# SARIMA: simple weekly seasonality \n",
    "model = SARIMAX(train[\"sales\"],\n",
    "                order=(1,1,1),\n",
    "                seasonal_order=(1,1,1,7),\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False)\n",
    "res = model.fit(disp=False)\n",
    "\n",
    "# forecast \n",
    "fcst = res.get_forecast(steps=len(test))\n",
    "test = test.assign(forecast=fcst.predicted_mean.values)\n",
    "\n",
    "# Plotting\n",
    "out_dir = processed.parent / \"results\" / \"figures\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(train.index, train[\"sales\"], label=\"Train\")\n",
    "plt.plot(test.index,  test[\"sales\"],  label=\"Test\")\n",
    "plt.plot(test.index,  test[\"forecast\"], label=\"Forecast\")\n",
    "plt.title(\"Sales Forecast vs Actual\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir / \"sales_forecast.png\")\n",
    "plt.close()\n",
    "\n",
    "# Metrics\n",
    "mae = (test[\"sales\"] - test[\"forecast\"]).abs().mean()\n",
    "print(f\"Mean Absolute Error: {mae:,.0f}\")\n",
    "print(f\"Saved figure -> {out_dir / 'sales_forecast.png'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4059dad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Forecast Summary\n",
      "- MAE (model): 59,561\n",
      "- MAE (seasonal-naive, 7D): 112,577  → Improvement vs naive: 47.1%\n",
      "- SMAPE: 6.7%\n",
      "- Payday diagnostic: residuals 27.2% lower → spikes largely captured.\n",
      "\n",
      "## Recommendations\n",
      "- Keep SARIMAX as baseline; add exogenous flags (payday, holidays, promos).\n",
      "- Track weekly MAE vs seasonal-naive to detect drift.\n",
      "\n",
      "## Ops/Scale\n",
      "- Rows: 125,497,040 | Days: 1,688 | ~1,914.9 MB | Runtime: 0.0s\n",
      "\n",
      "Saved summary -> c:\\Users\\Yizi\\New folder\\Payday-surge-favorita\\data\\results\\metrics\\forecast_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# Business insight \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "t0 = time.time()\n",
    "# Assumes df, daily, train, test already defined\n",
    "\n",
    "# 1) Test dates\n",
    "if isinstance(getattr(test, \"index\", None), pd.DatetimeIndex):\n",
    "    test_dates = test.index\n",
    "elif \"date\" in test.columns:\n",
    "    test_dates = pd.to_datetime(test[\"date\"])\n",
    "else:\n",
    "    test_dates = pd.to_datetime(daily[\"date\"].iloc[-len(test):].values)\n",
    "\n",
    "# 2) Actuals vs model forecast\n",
    "y_true = test[\"sales\"].astype(float).values\n",
    "y_hat  = test[\"forecast\"].astype(float).values\n",
    "\n",
    "# 3) Baseline: weekly seasonal-naive (t-7 calendar if possible, else repeat last 7)\n",
    "if isinstance(train.index, pd.DatetimeIndex) and isinstance(test_dates, pd.DatetimeIndex):\n",
    "    combined = pd.concat([train[\"sales\"], pd.Series(index=test_dates, dtype=float)], axis=0)\n",
    "    y_hat_sn = combined.shift(7).reindex(test_dates).ffill().fillna(0.0).values\n",
    "else:\n",
    "    last7 = train[\"sales\"].astype(float).tail(7).values\n",
    "    y_hat_sn = np.tile(last7, int(np.ceil(len(test) / 7)))[:len(test)]\n",
    "\n",
    "# 4) Metrics\n",
    "mae_model = float(np.mean(np.abs(y_true - y_hat)))\n",
    "mae_sn    = float(np.mean(np.abs(y_true - y_hat_sn)))\n",
    "impr_pct  = 100.0 * (1.0 - mae_model / mae_sn) if mae_sn > 0 else np.nan\n",
    "smape     = 100.0 * np.mean(2.0 * np.abs(y_hat - y_true) / (np.abs(y_true) + np.abs(y_hat) + 1e-9))\n",
    "\n",
    "# 5) Payday diagnostic (15th and month-end)\n",
    "dates = pd.DatetimeIndex(test_dates)\n",
    "paydayish = (dates.day == 15) | dates.is_month_end\n",
    "if paydayish.any() and (~paydayish).any():\n",
    "    resid = y_true - y_hat\n",
    "    mae_payday = float(np.mean(np.abs(resid[paydayish])))\n",
    "    mae_non    = float(np.mean(np.abs(resid[~paydayish])))\n",
    "    payday_lift = ((mae_payday / mae_non) - 1.0) * 100.0 if mae_non > 0 else np.nan\n",
    "else:\n",
    "    payday_lift = np.nan\n",
    "\n",
    "# 6) Ops/scale \n",
    "n_rows  = len(df)\n",
    "n_days  = len(daily)\n",
    "mem_mb  = df.memory_usage(index=False, deep=True).sum() / (1024**2)\n",
    "runtime = time.time() - t0\n",
    "\n",
    "# 7) Summary text (prints and saves to results/metrics/forecast_summary.txt)\n",
    "lines = [\n",
    "    \"## Forecast Summary\",\n",
    "    f\"- MAE (model): {mae_model:,.0f}\",\n",
    "    f\"- MAE (seasonal-naive, 7D): {mae_sn:,.0f}  → Improvement vs naive: {impr_pct:,.1f}%\",\n",
    "    f\"- SMAPE: {smape:,.1f}%\"\n",
    "]\n",
    "if np.isfinite(payday_lift):\n",
    "    if payday_lift > 10:\n",
    "        lines.append(f\"- Payday diagnostic: residuals {payday_lift:,.1f}% higher on 15th/month-end → add payday/holiday/promo signals.\")\n",
    "    elif payday_lift < -10:\n",
    "        lines.append(f\"- Payday diagnostic: residuals {abs(payday_lift):,.1f}% lower → spikes largely captured.\")\n",
    "    else:\n",
    "        lines.append(\"- Payday diagnostic: effect modest.\")\n",
    "lines += [\n",
    "    \"\",\n",
    "    \"## Recommendations\",\n",
    "    \"- Keep SARIMAX as baseline; add exogenous flags (payday, holidays, promos).\",\n",
    "    \"- Track weekly MAE vs seasonal-naive to detect drift.\",\n",
    "    \"\",\n",
    "    \"## Ops/Scale\",\n",
    "    f\"- Rows: {n_rows:,} | Days: {n_days:,} | ~{mem_mb:,.1f} MB | Runtime: {runtime:,.1f}s\"\n",
    "]\n",
    "summary = \"\\n\".join(lines)\n",
    "print(summary)\n",
    "\n",
    "# Save to results/metrics/forecast_summary.txt\n",
    "project_root = (processed_dir.parent if \"processed_dir\" in globals() else processed.parent)\n",
    "metrics_dir = project_root / \"results\" / \"metrics\"\n",
    "metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = metrics_dir / \"forecast_summary.txt\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary + \"\\n\")\n",
    "print(f\"\\nSaved summary -> {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df4401f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (model): 59561\n",
      "MAE (naive 7d): 69395\n",
      "SMAPE: 6.7 %\n",
      "Payday residual MAE: 44132\n",
      "Other days residual MAE: 60637\n",
      "Rows: 125,497,040 | Days: 1,688 | Runtime: 0.0s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# actuals vs forecast\n",
    "y_true = test[\"sales\"].values\n",
    "y_pred = test[\"forecast\"].values\n",
    "\n",
    "# simple 7-day naive baseline\n",
    "last_week = train[\"sales\"].tail(7).values\n",
    "naive_pred = np.tile(last_week, int(np.ceil(len(test)/7)))[:len(test)]\n",
    "\n",
    "# errors\n",
    "mae_model = np.mean(np.abs(y_true - y_pred))\n",
    "mae_naive = np.mean(np.abs(y_true - naive_pred))\n",
    "smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-9))\n",
    "\n",
    "print(\"MAE (model):\", round(mae_model))\n",
    "print(\"MAE (naive 7d):\", round(mae_naive))\n",
    "print(\"SMAPE:\", round(smape, 1), \"%\")\n",
    "\n",
    "# payday check (15th + month-end)\n",
    "resid = y_true - y_pred\n",
    "dates = test.index\n",
    "payday = (dates.day == 15) | dates.is_month_end\n",
    "\n",
    "if payday.any():\n",
    "    mae_payday = np.mean(np.abs(resid[payday]))\n",
    "    mae_other  = np.mean(np.abs(resid[~payday]))\n",
    "    print(\"Payday residual MAE:\", round(mae_payday))\n",
    "    print(\"Other days residual MAE:\", round(mae_other))\n",
    "    if mae_payday > mae_other * 1.1:\n",
    "        print(\"→ Spikes under-predicted on paydays (add payday/holiday flags).\")\n",
    "\n",
    "print(f\"Rows: {len(df):,} | Days: {len(daily):,} | Runtime: {time.time()-t0:.1f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
